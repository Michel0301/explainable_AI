This project demonstrates various explainable AI methods (attention-based explanations, LIME, SHAP, saliency maps, Grad-CAM and LayerCAM) for detecting bias in tiger recognition systems.

Usage:

1. Clone the repository: git clone https://github.com/Michel0301/explainable_AI.git
2. Install dependencies: pip install -r requirements.txt
3. Open the Jupyter Notebook “finalAssignment_wildlife_notebook.ipynb” using Jupyter Notebook or Google Colab.
4. Download the images.zip file and upload this to the Google Colab (or other IDE) directory environment
5. Run the Jupyter Notebook “finalAssignment_wildlife_notebook.ipynb” using Jupyter Notebook or Google Colab.

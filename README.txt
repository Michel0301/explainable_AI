This project demonstrates various explainable AI methods (attention-based explanations, LIME, SHAP, saliency maps, Grad-CAM and LayerCAM) for detecting bias in tiger recognition systems.

Usage:

1. Clone the repository: git clone https://github.com/Michel0301/explainable_AI.git
2. Install dependencies: pip install -r requirements.txt
3. Open and run the Jupyter Notebook “finalAssignment_wildlife_notebook.ipynb” using Jupyter Notebook or Google Colab.
